import streamlit as st
import cv2
import numpy as np
import threading
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Layer
import tempfile
import pygame
from gtts import gTTS
import time
import os

# ‡∏Ñ‡∏•‡∏≤‡∏™ Custom Layer
class CastLayer(Layer):
    def call(self, inputs):
        return inputs

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
@st.cache_resource
def load_emotion_model():
    model_path = "model_emotion.h5"
    return load_model(model_path, custom_objects={"Cast": CastLayer}, compile=False)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå
def predict_emotion(face, model):
    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
    face = cv2.equalizeHist(face)  
    face = cv2.resize(face, (48, 48)) / 255.0
    face = np.expand_dims(face, axis=-1)  
    face = np.expand_dims(face, axis=0)  

    predictions = model.predict(face)
    labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
    return labels[np.argmax(predictions)]

try:
    pygame.mixer.init(frequency=16000, size=-16, channels=1, buffer=2048)
except pygame.error as e:
    st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ pygame.mixer ‡πÑ‡∏î‡πâ: {e}")
    pygame.mixer.init()

is_speaking = False  

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏π‡∏î
def speak(text):
    global is_speaking

    if is_speaking:
        return
    
    is_speaking = True  
    def play_audio():
        global is_speaking
        tts = gTTS(text=text, lang="th")
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(temp_file.name)

        pygame.mixer.music.load(temp_file.name)
        pygame.mixer.music.play()

        while pygame.mixer.music.get_busy():
            time.sleep(0.1)  

        pygame.mixer.music.stop()
        time.sleep(0.5)  
        is_speaking = False  
        temp_file.close()

    threading.Thread(target=play_audio, daemon=True).start()


# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = load_emotion_model()

# ‚úÖ **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô show() ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ**
def show():
    st.title("üé≠ Real-time Emotion Detection")
    run = st.checkbox("üì∏ ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á")

    emotion_speech = {
        "Angry": "‡∏Ñ‡∏∏‡∏ì‡∏î‡∏π‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏ô‡∏∞ ‡πÉ‡∏à‡πÄ‡∏¢‡πá‡∏ô‡πÜ ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö",
        "Disgust": "‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏Ç‡∏¢‡∏∞‡πÅ‡∏Ç‡∏¢‡∏á",
        "Fear": "‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏Å‡∏•‡∏±‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤",
        "Happy": "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏î‡∏π‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç",
        "Sad": "‡∏Ñ‡∏∏‡∏ì‡∏î‡∏π‡πÄ‡∏®‡∏£‡πâ‡∏≤ ‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏∞",
        "Surprise": "‡∏Ñ‡∏∏‡∏ì‡∏î‡∏π‡∏ï‡∏Å‡πÉ‡∏à ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏≠",
        "Neutral": "‡∏Ñ‡∏∏‡∏ì‡∏î‡∏π‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©"
    }

    if run:
        cap = cv2.VideoCapture(0)
        stframe = st.empty()
        stop_button = st.button("üî¥ ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á")

        last_emotion = None  
        face_detected = False  

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or stop_button:
                st.warning("üìå ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß")
                break

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
            faces = face_cascade.detectMultiScale(gray, 1.3, 5)

            if len(faces) > 0:
                face_detected = True  
            else:
                face_detected = False  
                last_emotion = None  

            for (x, y, w, h) in faces:
                face = frame[y:y+h, x:x+w]
                emotion = predict_emotion(face, model)

                if emotion != last_emotion:
                    if not is_speaking:
                        speak(emotion_speech.get(emotion, ""))
                        last_emotion = emotion  

                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(frame, emotion, (x + 5, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)

            stframe.image(frame, channels="BGR")
        
        cap.release()
    else:
        st.warning("üìå ‡∏Å‡∏î‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô") 